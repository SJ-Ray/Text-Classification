{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:14:25.897542Z",
     "start_time": "2022-01-05T07:14:13.108852Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, concatenate, Activation,Dropout, Input, LSTM, Flatten, Embedding\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "from gensim.utils import simple_preprocess\n",
    "import csv\n",
    "import fasttext\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:14:26.054960Z",
     "start_time": "2022-01-05T07:14:25.903387Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suraj\\anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.txt', sep=\"__\", header=None)\n",
    "data[2] = data[2].apply(lambda x: '__label__' + x)\n",
    "data.drop(1,axis=1,inplace=True)\n",
    "data.rename(columns={0:'Consumer complaint narrative',2:'Target'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:17.578506Z",
     "start_time": "2022-01-05T07:17:17.570954Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:18.187644Z",
     "start_time": "2022-01-05T07:17:18.168394Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    #text = \" \".join(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\[.*?\\]’', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = text.replace(\"’\",\"\")\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:18.718939Z",
     "start_time": "2022-01-05T07:17:18.683680Z"
    }
   },
   "outputs": [],
   "source": [
    "more_stp_words = ['xxxx', 'xxxxxxxx', 'report', 'reporting', 'loan', \n",
    "                  'company', 'debt', 'bank', 'received', 'accounts', 'sent', 'would', 'also', 'letter', \n",
    "                  'days', 'never', 'told','an','am','said','dont','like','dint','know','ive','got'\n",
    "                 ,'could','see','havent','im','going','looked','through','tell','yeh',\n",
    "                 'go','back','get','yer','would','never','seen','something','else','next','day','years'\n",
    "                 ,'didnt','look','thousand','one','ever','even','though','although','every','time','make','sure'\n",
    "                 ,'told','minutes','minute','hour','where','else','no','oh','ten','year','front','door','wandering','nine',\n",
    "                 'ten','points','fifty','hundred','twenty','gotten','fell','asleep','yes','trying','find','one','two','three'\n",
    "                 ,'four','thirteen','clock','give','us','youd','expect','picked','lived','number','twice','youknowwho','think',\n",
    "                  'hed','harrys','well','around','items','due', 'please','sba','end','find','route','continues','look','cfpb',\n",
    "                 'matter','look','said','good','tells','also','connected','help','business','email','working','many','work',\n",
    "                 'still','want','talk','issues','issue','take','form','file','fair','consumer','reasonable','means','therefore',\n",
    "                 'inquiries','cc','shall','call','date','open','attn','since','xx','credit','card','citi','citibank',\n",
    "                 'account','called','made','information','balance']\n",
    "stop_words.extend(more_stp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:19.111178Z",
     "start_time": "2022-01-05T07:17:19.088211Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_clean(texts):\n",
    "    toks = word_tokenize(texts)\n",
    "    stp = [word for word in toks if word not in stop_words]\n",
    "    #stpr = ' '.join(stp)\n",
    "    return stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:22.139991Z",
     "start_time": "2022-01-05T07:17:19.584672Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11048/2215816358.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Consumer complaint narrative'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Consumer complaint narrative'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11048/1606737322.py\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#text = \" \".join(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\[.*?\\]’'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_sample)):\n",
    "    df_sample['Consumer complaint narrative'][i] = clean_text(df_sample['Consumer complaint narrative'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:22.146979Z",
     "start_time": "2022-01-05T07:17:22.146979Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_sample)):\n",
    "    df_sample['Consumer complaint narrative'][i] = final_clean(df_sample['Consumer complaint narrative'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:22.150928Z",
     "start_time": "2022-01-05T07:17:22.150928Z"
    }
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "    \n",
    "t.fit_on_texts(df_sample['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-05T07:17:22.813157Z",
     "start_time": "2022-01-05T07:17:22.795780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "leng = []\n",
    "for i in df_sample['Consumer complaint narrative']:\n",
    "    leng.append(len(i))\n",
    "    \n",
    "import numpy as np\n",
    "index_max = np.argmax(leng)\n",
    "print(max(leng))\n",
    "print(index_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_essay = t.texts_to_sequences(df_sample['Consumer complaint narrative'])\n",
    "\n",
    "X_essay = pad_sequences(X_essay, maxlen=341, padding='pre')\n",
    "# here 340 as 1 space counts 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917495 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "f = open('/Users/akashbhoite/Downloads/glove.42B.300d.txt', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "\n",
    "    values = line.split()\n",
    "\n",
    "    word = values[0]\n",
    "\n",
    "    coefs = np.asarray(values[1:])\n",
    "\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))#create a weight matrix for words in training docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "\n",
    "for word, i in t.word_index.items():\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('train.txt',pretrainedVectors=embeddings_index,wordNgrams = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test('test.txt')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions[0]).flatten()\n",
    "target = df_sample['Target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(target,predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
